{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JYLdevGhh65"
   },
   "source": [
    "# Impelemntation of Recency Aware CF Filtering for the Next Basket Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmslMMHHiShE"
   },
   "source": [
    "## 0-Modules & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXCY6CpLh3d7"
   },
   "source": [
    "For a start we going to install and imports some important python modules we going to work with:\n",
    "* We first, using the package installer Pip, we install similaripy module. \n",
    "* **similaripy** : is a fast Python implementation for similarity algorithms using sparse matrices useful in CF-Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "F4E7piBWAQeO",
    "outputId": "a8fccff8-40d2-4983-aa89-9c1274e07249"
   },
   "outputs": [],
   "source": [
    "!pip install similaripy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFJPW9UgiroM"
   },
   "source": [
    "* Once the installation is done, we imports the needed python modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebY9j-WvGlvC"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# In order to deal with large sparse matrix we need to compresse them using\n",
    "# the sparse sub_module of scipy lib\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import ndcg_score\n",
    "import similaripy as sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93vbt5TjjCbH"
   },
   "source": [
    "## If Working with Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tl-IgjZmMjY0",
    "outputId": "52b4cdc7-e037-4a37-f095-72dd5e16c8d9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbC5_VFQBj3h"
   },
   "source": [
    "And navigate where data is stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9KcdsIkqMgIp",
    "outputId": "dcbf7ff5-f45e-4d1a-de5a-67e09028d9d8"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/\"My Drive\"/\"For colab\"/RecomdSys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQoULRn-cwMb"
   },
   "source": [
    "## I- Data Manipulation:\n",
    "In this, first section, I started by manipulating the data we've been given, Instacart & Dunnhumby datasets, by creating two parsers :\n",
    "* instacartParser\n",
    "* DunnhumbyParser (Not yet)\n",
    "\n",
    "In parsers, I've done some light preprocessing:\n",
    "  \n",
    "* Removing all items that appear in less than some number of baskets (aka item_threshold).\n",
    "* Excluding users with less than some number of baskets (aka user_threshold)\n",
    "\n",
    "And in order to deal with limitation of my computer setting (Ram 16GB/i7-4501U CPU), I also added a subdata percentage parametre to extract a certain amount of data to deal with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIB8RYPOQY3P"
   },
   "outputs": [],
   "source": [
    "def instacartParser(dataPath, item_threshold=10, basket_threshold=2, subdata=0.1,verbose=True):\n",
    "  '''\n",
    "  IN:\n",
    "    dataPath : os path to instacart data\n",
    "    item_threshold : (default = 10 basket)\n",
    "    basket_threshold : (default = 2 basket)\n",
    "    subdata : (default: 10% of data)\n",
    "    verbose: Boolean, (default:True)\n",
    "  OUT:\n",
    "    df_train : DataFrame where columns = [BID, UID, order,articles]\n",
    "    dev_set, test_set : user-baskets items dict like {UID -> [PID,...], UID -> [PID,...], ...}\n",
    "    df_products: DataFrame where columns = [PID, description, department, category]\n",
    "  '''\n",
    "  if verbose :\n",
    "    start = time.time()\n",
    "    # Start Time for calculating execution time\n",
    "    print('Constructing products DataFrame object ...')\n",
    "\n",
    "  # Read products.csv\n",
    "  df_products= pd.read_csv(os.path.join(path,\"products.csv\"))\n",
    "  df_products.columns = ['PID', 'description', 'categoryId', 'departmentId']\n",
    "  # Read departments.csv and merge\n",
    "  tmp = pd.read_csv(os.path.join(path,\"departments.csv\"))\n",
    "  tmp.columns = ['departmentId', 'department']\n",
    "  df_products = pd.merge(df_products, tmp, on='departmentId')\n",
    "  # Read aisles.csv and merge \n",
    "  tmp = pd.read_csv(os.path.join(path,'aisles.csv'))\n",
    "  tmp.columns = ['categoryId', 'category']\n",
    "  df_products = pd.merge(df_products, tmp, on='categoryId')[['PID', 'description','department','category']]\n",
    "  del tmp\n",
    "\n",
    "  # preprocessing\n",
    "  if verbose:\n",
    "    print('Constructing order_products DataFrame object ...')\n",
    "\n",
    "  df_order_products_prior = pd.read_csv(os.path.join(path,\"order_products__prior.csv\"))\n",
    "  df_order_products_train = pd.read_csv(os.path.join(path,\"order_products__train.csv\"))\n",
    "  df_order_products = pd.concat([df_order_products_prior, df_order_products_train])[['order_id', 'product_id']]\n",
    "  df_order_products.columns= ['BID','PID']\n",
    "  del df_order_products_prior, df_order_products_train,\n",
    "\n",
    "  if verbose:\n",
    "    print('Filtring items ...')\n",
    "  # Remouving all items that appears in less than item_threshold baskets\n",
    "  products_count = df_order_products['PID'].value_counts()\n",
    "  df_order_products= df_order_products.loc[df_order_products['PID'].isin(products_count[products_count >= item_threshold].index)]\n",
    "  del products_count\n",
    "  # Updating production list \n",
    "  pd.merge(df_products,df_order_products['PID'],on='PID')\n",
    "\n",
    "  if verbose:\n",
    "    print('Reading Users order.csv file ...')\n",
    "  df_orders = pd.read_csv(os.path.join(path,\"orders.csv\"))[['order_id', 'user_id', 'order_number', 'eval_set']]\n",
    "  df_orders.columns = ['BID','UID','order', 'set']\n",
    "\n",
    "  if verbose:\n",
    "    print('Filtring Users...')\n",
    "    print('Getting',subdata*100,'% of our dataset...')\n",
    "  # User filtring\n",
    "  # Remouving users with less than basket_threshold baskets\n",
    "  user_count = df_orders['UID'].value_counts()\n",
    "  user_filter = user_count[(user_count>=basket_threshold)&(np.random.rand(len(user_count))< subdata)]\n",
    "  del user_count\n",
    "  df_orders = df_orders[df_orders['UID'].isin(user_filter.index)]\n",
    "  del user_filter\n",
    "\n",
    "  # Reset UID \n",
    "  if verbose:\n",
    "    print('Reset UID indexing')\n",
    "\n",
    "  user_dict = dict(zip(df_orders['UID'].unique(),range(len(df_orders['UID'].unique()))))\n",
    "  df_orders['UID'] = df_orders['UID'].map(user_dict)\n",
    "  del user_dict\n",
    "  # reset product index\n",
    "  df_order_products = df_order_products.loc[df_order_products['BID'].isin(df_orders['BID'].unique())]\n",
    "  df_products = df_products[df_products['PID'].isin(df_order_products['PID'].unique())]  \n",
    "  product_dict = dict(zip(df_order_products['PID'].unique(),range(len(df_order_products['PID'].unique()))))\n",
    "  df_products['PID'] = df_products['PID'].map(product_dict)\n",
    "  df_order_products['PID'] = df_order_products['PID'].map(product_dict)\n",
    "  del product_dict\n",
    "\n",
    "  # Join Tables\n",
    "  if verbose:\n",
    "    print('Joining tables ...')\n",
    "  df_data = pd.merge(df_orders, df_order_products, on= 'BID')\n",
    "  del df_orders, df_order_products\n",
    "\n",
    "  if verbose:\n",
    "    print(\"spliting data ...\")\n",
    "  # Setting last baskets as dev/test sets\n",
    "  last_basket_indexes = df_data.iloc[df_data.groupby(['UID'])['order'].idxmax()]['BID'].values\n",
    "  df_data.loc[df_data['BID'].isin(last_basket_indexes),'set']='test'\n",
    "  df_data.loc[df_data['set']=='prior', 'set'] = 'train'\n",
    "  del last_basket_indexes\n",
    "\n",
    "  # train test split data\n",
    "  df_split = df_data[df_data['set']=='test'].groupby(by=['UID'])['PID'].apply(list).reset_index(name='articles')\n",
    "  msk = (np.random.rand(len(df_split))<0.5)\n",
    "  df_dev, df_test = df_split[msk], df_split[~msk]\n",
    "  del df_split\n",
    "\n",
    "  df_train = df_data[df_data['set']=='train'][['UID','BID','order','PID']]\n",
    "  dev_set = dict(zip(df_dev['UID'],df_dev['articles']))\n",
    "  test_set = dict(zip(df_test['UID'],df_test['articles']))\n",
    "\n",
    "  del msk, df_dev, df_test\n",
    "  # simple check\n",
    "  assert (len(dev_set)+len(test_set))==(df_data['UID'].unique().shape[0])\n",
    "  del df_data\n",
    "\n",
    "  if verbose:\n",
    "    print(\"processing took {0:.1f} sec\".format(time.time() - start))\n",
    "  \n",
    "  return df_train, dev_set, test_set, df_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSW2oGWmOJZj"
   },
   "source": [
    "## II- Helper functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Py1JLDV5AQd7"
   },
   "source": [
    "* Top-k Predection \n",
    "* Evaluation using the nDGC@k score provided by sklearn.metrics python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBEE6gQpAQd8"
   },
   "outputs": [],
   "source": [
    "def top_n(row, n):\n",
    "    '''\n",
    "    IN : \n",
    "      row : 1-D ndarray\n",
    "      n   : int, number of top items\n",
    "    OUT:\n",
    "      top_values  : 1-D ndarray, Represent Top-n scores of the given row\n",
    "      top_indices : 1-D ndarray, Represent Top-n users indices of the given row\n",
    "    '''\n",
    "    # Get user indices to sort the given row\n",
    "    top_indices = row.argsort()[-n:][::-1]\n",
    "    # Use the top_indices to get top_values score\n",
    "    top_values  = row[top_indices]\n",
    "    return top_values, top_indices\n",
    "\n",
    "def prediction(predMat, k):\n",
    "    '''\n",
    "    In :\n",
    "      predMat : the predection matrix \n",
    "      {UWPop, UB-CF, IB-CF }with/out recency (@r)\n",
    "    Out :\n",
    "      score, pred : ndarray of shape =(n_users, k)\n",
    "      retun the top-k score and predection matrix\n",
    "    '''\n",
    "    n_users = predMat.shape[0]\n",
    "    score = np.zeros((n_users, k))\n",
    "    pred  = np.zeros((n_users, k))\n",
    "    for i in range(n_users):\n",
    "        score[i], pred[i] = top_n(predMat[i],k)\n",
    "    return score.astype('float64'), pred.astype('int64')\n",
    "\n",
    "def evaluation(score, pred, test_set, dev_set, k=5):\n",
    "    '''\n",
    "    Calculate the ndgs score for both test/dev sets for a giver user-item score and predection matrix.\n",
    "    IN:\n",
    "      score, pred: (n_user, m_items) ndarray matrix\n",
    "      test_set, dev_set : user-baskets items dict like {UID -> [PID,...], UID -> [PID,...], ...}\n",
    "      k :(default fixed to 5)    \n",
    "    OUT:\n",
    "      test_ndcg_score, dev_ndcg_score : type:int, evaluation metric for both test and dev set respectively  \n",
    "    '''\n",
    "    # Get the test and dev set User IDs\n",
    "    test_keys = test_set.keys()\n",
    "    dev_keys  = dev_set.keys()\n",
    "    # Construct the True_relecvance and score vectors\n",
    "    true_relevance_test = np.asarray([np.isin(pred[key],test_set[key]).astype(int) for key in test_keys])\n",
    "    true_relevance_dev  = np.asarray([np.isin(pred[key],dev_set[key]).astype(int) for key in dev_keys])\n",
    "    score_test = score[list(test_keys)]\n",
    "    score_dev  = score[list(dev_keys)]\n",
    "    # Calculate the ndgc@k evaluation metric \n",
    "    test_ndcg_score = ndcg_score(true_relevance_test, score_test, k)\n",
    "    dev_ndcg_score  = ndcg_score(true_relevance_dev, score_dev, k)\n",
    "    return test_ndcg_score, dev_ndcg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pSECoplmvEr2"
   },
   "source": [
    "## III- Methods implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LzvWDO1nUtb6"
   },
   "source": [
    "Begin by cleaning and splitting Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "tHW2LUSXbLOb",
    "outputId": "cc8a5bea-b6e1-4c7a-cbee-61015f39500a"
   },
   "outputs": [],
   "source": [
    "# here we hard-coded the instacart dataset's path\n",
    "path = \"instacart\"\n",
    "# We return df_products for further uses like doing some analysis \n",
    "df_train, dev_set, test_set, df_products = instacartParser(path, item_threshold=10, basket_threshold=2, subdata=0.05,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YljzZJ_VGBJa"
   },
   "source": [
    "Show some dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "i3wlFhiEWHPp",
    "outputId": "3a99f78d-321b-41a9-9be7-24463bd7f4ed"
   },
   "outputs": [],
   "source": [
    "n_users = df_train.UID.unique().shape[0]\n",
    "n_items = df_products['PID'].unique().shape[0]\n",
    "n_baskets = df_train.BID.unique().shape[0]\n",
    "print('n_users:',n_users)\n",
    "print('n_items:',n_items)\n",
    "print('n_baskets:',n_baskets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ73y67sWSJE"
   },
   "source": [
    "### 1-User-Wise Popularity Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OI16J_8AWKqy"
   },
   "outputs": [],
   "source": [
    "def uwPopMat(df_train, n_items, recency=0):\n",
    "  '''\n",
    "    Calculate the user popularity matrix with the given recency window\n",
    "  '''\n",
    "  n_users = df_train.UID.unique().shape[0]\n",
    "  if (recency>0):\n",
    "    # Get the number of user baskets Bu \n",
    "    BUCount = df_train.groupby(['UID'])['order'].max().reset_index(name='Bu')\n",
    "    # Calculate the denominator which equal to Min(recency,Bu) for each user\n",
    "    BUCount['denominator'] = np.minimum(BUCount['Bu'],5)\n",
    "    # Calculater the order index, form where we start counting item appearance in recent orders   \n",
    "    BUCount['startindex'] = np.maximum(BUCount['Bu']-5,0)\n",
    "    # Calcualte item appearance in recent orders   \n",
    "    tmp = pd.merge(BUCount, df_train,on='UID')[['UID','PID','order','startindex']]\n",
    "    tmp = tmp.loc[(tmp['order']>=tmp['startindex'])==True].groupby(['UID','PID'])['order'].count().reset_index(name='numerator')\n",
    "    tmp = pd.merge(BUCount[['UID','denominator']],tmp,on='UID')\n",
    "    # finally calculate the recency aware user-wise popularity\n",
    "    tmp['Score'] = tmp['numerator']/tmp['denominator']\n",
    "  else : \n",
    "    # Calculate user-wise popularity for each item\n",
    "    BUCount = df_train.groupby(['UID'])['order'].max().reset_index(name='Bu')\n",
    "    BUICount = df_train.groupby(['UID','PID'])['BID'].count().reset_index(name='Bui')\n",
    "    tmp = pd.merge(BUICount, BUCount, on='UID')\n",
    "    del BUICount\n",
    "    tmp['Score'] = tmp['Bui']/tmp['Bu']\n",
    "    del BUCount\n",
    "    # get the 3 columns needed to construct our user-wise Popularity matrix\n",
    "  df_UWpop =  tmp[['UID','PID','Score']]   \n",
    "  del tmp\n",
    "  # Generate user-wise popularity matrix in COOrdinate format\n",
    "  UWP_mat = sparse.coo_matrix((df_UWpop.Score.values, (df_UWpop.UID.values, df_UWpop.PID.values)), shape=(n_users,n_items))\n",
    "  del df_UWpop\n",
    "  return sparse.csr_matrix(UWP_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4PcUNbwvRwU"
   },
   "source": [
    "#### A - UWPop Without recency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laivN9ZLvX5y"
   },
   "source": [
    "##### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LL_53mp15AdY"
   },
   "outputs": [],
   "source": [
    "n_items = df_products['PID'].unique().shape[0]\n",
    "UWP_mat = uwPopMat(df_train, n_items, recency=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uAuAMES7IBgp",
    "outputId": "9854bc6f-8ccc-4b7f-c814-9e8fcc811ac6"
   },
   "outputs": [],
   "source": [
    "UWP_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdOzw-MkvBN5"
   },
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rdUzOWu-AQeF",
    "outputId": "43d347a0-d4ba-4c20-d343-f442280e6ba5"
   },
   "outputs": [],
   "source": [
    "# prediction \n",
    "score, pred = prediction(UWP_mat.toarray(), k=10)\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=10)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FWUhxpqbe4m"
   },
   "source": [
    "#### b-UWPop with recency (UWPop@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VeHknWgvm6k"
   },
   "source": [
    "##### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvcTpTaIbbg3"
   },
   "outputs": [],
   "source": [
    "n_items = df_products['PID'].unique().shape[0]\n",
    "UWPr_mat = uwPopMat(df_train, n_items, recency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F76yfDCKNIT4",
    "outputId": "69137a9b-cc22-4857-a2d8-dc72b98afff6"
   },
   "outputs": [],
   "source": [
    "UWPr_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFf3igk6vpfe"
   },
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ONGlK-upAQeL",
    "outputId": "b83cf457-520c-421e-b8b5-8e61edf8d89d"
   },
   "outputs": [],
   "source": [
    "# predection\n",
    "score, pred = prediction(UWPr_mat.toarray(), k=10)\n",
    "# Evaluation\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=10)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKW9cnD8AQeQ"
   },
   "source": [
    "### sparse martrix\n",
    "Due to the high memory consumption and in order to build the User-user (item-item) similarity matrix, which is known to be highly sparse. we always return a sparse UWPop (user-wise popularity) matrix, using the scipy-python sub-module **sparse** and **similaripy** python module.\n",
    "\n",
    "First we start by getting the compressed UWPop Matrix (with and without recency) in CSR(Compressed Sparse Row) format. Then calculate the similarity matrix in order to get the final prediction and evaluate our model. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XONOQVygcB5l"
   },
   "source": [
    "### 2-Popularity-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAUyJRtEHWaj"
   },
   "source": [
    "#### 2.1- User Popularity-based CF (UP-CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yulJNe8AE1sL"
   },
   "outputs": [],
   "source": [
    "def upcf(df_train, UWP_sparse, n_items, alpha = 0.25 ,q=5, k=10):\n",
    "  n_users = df_train['UID'].unique().shape[0]\n",
    "  df_user_item = df_train.groupby(['UID','PID']).size().reset_index(name=\"bool\")[['UID','PID']]\n",
    "  # Generate the User_item matrix using the parse matrix COOrdinate format.\n",
    "  userItem_mat = sparse.coo_matrix((np.ones((df_user_item.shape[0])), (df_user_item.UID.values, df_user_item.PID.values)), shape=(n_users,n_items))\n",
    "  # Calculate the asymmetric similarity cosine matrix \n",
    "  userSim = sim.asymmetric_cosine(sparse.csr_matrix(userItem_mat), alpha=0.25, k=10)\n",
    "  # recommend k items to users\n",
    "  user_recommendations = sim.dot_product(userSim.power(5), UWP_sparse, k=10)\n",
    "  return user_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3BmHHNRx0KK"
   },
   "source": [
    "##### A- UP-CF without recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ezsIL86qQ_C1",
    "outputId": "b0a8bd3b-c50e-4fcd-9964-f322ca9fbb3d"
   },
   "outputs": [],
   "source": [
    "n_items = df_products['PID'].unique().shape[0]\n",
    "user_recommendations = upcf(df_train, UWP_mat, n_items, alpha = 0.25, q=5, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fqjs5RW2L2wC",
    "outputId": "69eeff46-27c6-4c11-df64-2ee8acfb8067"
   },
   "outputs": [],
   "source": [
    "# predection\n",
    "score, pred = prediction(user_recommendations.toarray(), k=10)\n",
    "del user_recommendations\n",
    "# Evaluation\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=10)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XBCUhqZySZU"
   },
   "source": [
    "##### B- UP-CF with recency (UP-CF@r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dVqemT3lx72N",
    "outputId": "4a7df2b8-4f35-4db3-f139-11082602032d"
   },
   "outputs": [],
   "source": [
    "n_items = df_products['PID'].unique().shape[0]\n",
    "user_recommendations = upcf(df_train, UWPr_mat, n_items, alpha = 0.25, q=5, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nCdrhzGVYLxh",
    "outputId": "174c4710-0ff9-4c0b-d11e-60269db680e0"
   },
   "outputs": [],
   "source": [
    "user_recommendations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mrAiIV5EPtJa",
    "outputId": "0b524692-a3ba-43aa-d2dd-e0cc8693e400"
   },
   "outputs": [],
   "source": [
    "# predection\n",
    "score, pred = prediction(user_recommendations.toarray(), k=5)\n",
    "del user_recommendations\n",
    "# Evaluation\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=5)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "540UARffzlSQ"
   },
   "source": [
    "####2.2-Item popularity-based Collaborative Filtring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYUjsubo7AQE"
   },
   "outputs": [],
   "source": [
    "def ipcf(df_train, UWP_sparse, n_items,alpha = 0.25, q=5, k=10):\n",
    "  # Construct the item-basket sparse matrix \n",
    "  idMax_basket = df_train.BID.max()+1\n",
    "  item_basket_mat = sparse.coo_matrix((np.ones((df_train.shape[0]),dtype=int), (df_train.PID.values, df_train.BID.values)), shape=(n_items,idMax_basket))\n",
    "  # Convert it to Compressed Sparse Row format to exploit its efficiency in arithmetic operations \n",
    "  sparse_mat = sparse.csr_matrix(item_basket_mat)\n",
    "  # Caculate the Asymetric Cosine Similarity matrix\n",
    "  itemSimMat = sim.asymmetric_cosine(sparse_mat, None, alpha, k)\n",
    "  # recommend k items to users\n",
    "  UWP_sparse.shape, itemSimMat.shape\n",
    "  user_recommendations = sim.dot_product(UWP_sparse, itemSimMat.power(q), k)\n",
    "  return user_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtCTCg618pNL"
   },
   "source": [
    "##### A- IP-CF without recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7gZq7loD8x7k",
    "outputId": "4fae7692-2044-48d9-8fb9-d8cb361430ae"
   },
   "outputs": [],
   "source": [
    "user_recommendations = ipcf(df_train, UWP_mat, n_items, alpha = 0.25, q=5, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "OYu0WiNp0B6K",
    "outputId": "f4056dd9-b467-4a12-f502-3632707d1516"
   },
   "outputs": [],
   "source": [
    "# predection\n",
    "score, pred = prediction(user_recommendations.toarray(), k=5)\n",
    "del user_recommendations\n",
    "# Evaluation\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=5)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tglOViZGAQe1"
   },
   "source": [
    "##### B- IP-CF with recency (IP-CF@r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VsPZJxUbAaFv",
    "outputId": "f8db9d97-61a9-4596-a2cb-0e0f01eff0f8"
   },
   "outputs": [],
   "source": [
    "# Use the UWP@r sparse matrix\n",
    "user_recommendations = ipcf(df_train, UWPr_mat, n_items, alpha = 0.25, q=5, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Tm62Vrk6AcMO",
    "outputId": "21597f05-b789-4a64-8acb-79b323958154"
   },
   "outputs": [],
   "source": [
    "# predection\n",
    "score, pred = prediction(user_recommendations.toarray(), k=5)\n",
    "del user_recommendations\n",
    "# Evaluation\n",
    "test_ndcg, dev_ndcg = evaluation(score, pred, test_set, dev_set, k=5)\n",
    "print(\"test score:\",test_ndcg,\"\\n dev score:\",dev_ndcg)\n",
    "del score,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiL75g7QkxPv"
   },
   "source": [
    "## IV- References:\n",
    "\n",
    "**scipy sparse matrix**\n",
    "\n",
    "ref\\[0\\] : https://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\n",
    "ref\\[1\\] : https://stackoverflow.com/questions/36135927/get-top-n-items-of-every-row-in-a-scipy-sparse-matrix\n",
    "\n",
    "ref\\[2\\] : https://stackoverflow.com/questions/31790819/scipy-sparse-csr-matrix-how-to-get-top-ten-values-and-indices\n",
    "\n",
    "ref\\[4\\] : https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\n",
    "**similaripy**\n",
    "\n",
    "ref\\[5\\] : https://pypi.org/project/similaripy/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RecencyAwareCF.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
